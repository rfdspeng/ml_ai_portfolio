{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea12c521-7dbb-4eb9-8109-d52d40f9e477",
   "metadata": {},
   "source": [
    "# <u>What is the flow?</u>\n",
    "\n",
    "The user asks or says something.\n",
    "* Non-agentic: Encode the user input, retrieve the top results from the vector DB, pass it into the LLM, and then the LLM responds with the given context. For even more control, I can prompt the user with dropdown menus for metadata, although at this point, it's really limiting the deep learning application.\n",
    "* Agentic: The LLM is an agent. It will figure out what it should do - it can respond immediately or it will take the user input and format it into something else. For example, the LLM retrieves the important information from the user input - genre, ratings threshold, author, etc. Then, the LLM creates a new text from the important information, encodes the new text, and queries the vector DB, along with metadata filters.\n",
    "* Semi-agentic: I prompt the LLM with something like \"You are a book recommender. If the user requests something specific, extract the key information from the user request in this format: {\"genre\": some_genre, \"author\": some_author, etc.}. If the user request does not contain a field, like \"author\", do not include it in the dictionary.\" The LLM pulls the key information, and then I encode the user input and query the database with the metadata. So the LLM is not an agent, but it helps with parsing the key information from the user input.\n",
    "\n",
    "How do I include weighting by number of ratings and rating?\n",
    "\n",
    "Let's start from low complexity and build our way up.\n",
    "\n",
    "On the machine learning side:\n",
    "* 1: Embed user query, retrieve, feed to chatbot. Flask/Fast and Streamlit locally hosted. Where to store conversation history? I don't need backend with Streamlit?\n",
    "* 2: Embed user query, retrieve, feed to chatbot. Containerize and deploy using AWS Lambda and AWS App Runner.\n",
    "* 3: Use LLM to extract information from user query for metadata filtering via prompting. Reformat user query, embed, retrieve, feed to chatbot. If user query is not a true query, do not retrieve. Create tools for agentic behavior. Like a function that returns available genres.\n",
    "* 4: Fine-tune smaller model to extract information from user query. Use LLM to provide training samples.\n",
    "\n",
    "On the deployment side:\n",
    "\n",
    "1. Use Streamlit to locally host and to deploy to cloud. If you're only using OpenAI's API for retrieval and generation, you don't need more than this.\n",
    "2. Add Flask or fastAPI for the backend and integrate with Streamlit.\n",
    "3. Containerize your code, which includes Flask or fastAPI, the retrieval code, the chatbot code, and deploy to EC2 instance.\n",
    "4. Deploy code using \n",
    "5. Containerize the retrieval agent and the chatbot and deploy to EC2 instance. \n",
    "6. Use Flask and fastAPI for the backend\n",
    "\n",
    "Book recommendation system using RAG and OpenAI embedding model and chat completion. Flow: User query -> (optional) intent classifier -> (optional) extract information from user query -> embed user query -> retrieve relevant books from vector DB -> pass user query and book information to chatbot -> chatbot responds and updates conversation history.\n",
    "\n",
    "Implementation plan, from low to high complexity:\n",
    "\n",
    "1. Directly embed user query and retrieve (no extraction of key information; embed the user query as-is). Conversation is stored in `st.session_state` (only persists for the duration of the user's session). Use Streamlit to deploy (both locally and to Streamlit Cloud).\n",
    "    * Deployed locally; Streamlit Cloud failed and now it won't even let me try to create the app\n",
    "2. Before retrieval, use LLM with prompting to extract key information from the user query and reformat user query before embedding. Add metadata filtering to retrieval.\n",
    "3. Add fastAPI for the backend.\n",
    "4. Containerize the code (fastAPI, Streamlit, OpenAI API calls) and deploy to EC2 instance.\n",
    "5. Containerize the backend and deploy using AWS Lambda. Deploy Streamlit using Streamlit Cloud, calling AWS Lambda functions via API Gateway.\n",
    "6. Add intent classifier as the first step to classify user query into relevant and non-relevant. Relevant queries are requests for book recommendations; all others are non-relevant. Use LLM to generate synthetic data, then fine-tune the encoder model (like BERT) for intent classification, using MLFlow and AWS SageMaker. For non-relevant queries, skip directly to chatbot (no retrieval).\n",
    "    * Completely new, unrelated query (retrieval)\n",
    "    * Follow-up query to the previous query (no retrieval)\n",
    "    * New, related query, e.g. asking for similar books to the previous results (retrieval)\n",
    "    * Non-relevant query (no retrieval)\n",
    "7. Add NER classifier to extract key information from the user query and reformat user query before embedding (this replaces the LLM with prompting from step 2). Use LLM to generate synthetic data, then fine-tune encoder model for NER classification, using MLFlow and AWS SageMaker.\n",
    "8. Note that loading these models for each Lambda will increase latency.\n",
    "9. For long sessions, reduce conversation history length via summarization or some other method.\n",
    "10. To maintain conversation history across sessions, store conversation history to AWS DynamoDB.\n",
    "11. Query expansion?\n",
    "12. Re-ranking?\n",
    "13. Add LangChain at some point\n",
    "14. CI/CD with GitHub Actions. Unittest and pytest libraries. Streamlit recommended using pytest to test changes.\n",
    "15. Use pip instead of conda\n",
    "15. Kubernetes?\n",
    "* Stress test concurrent users for backend\n",
    "* Profile app speed with and without asynchronous programming\n",
    "16. Stretch goal - customize Streamlit frontend with HTML/CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b344843",
   "metadata": {},
   "source": [
    "# <u>Waqas recommendations</u>\n",
    "* Backend Enhancements: Use AWS Lambda for backend implementation to gain experience with serverless architectures. I got Lambda working (synchronous invocation through API Gateway).\n",
    "    * DynamoDB to store user chat history\n",
    "* Can I add Nginx to my stack (server-based architecture)? https://github.com/luntaixiax/wcd-web-nginx\n",
    "* NLP Training and Deployment: Utilize SageMaker for training NLP systems, such as classification or Named Entity Recognition (NER), and also for deploying these models.\n",
    "* Open-Source Models: Experiment with open-source models like Hugging Face's 1B, 3B, or 7B parameters for text generation tasks.\n",
    "* Embedding Models: Leverage any open-source embedding models to enrich recommendations or search functionalities.\n",
    "* Advanced Recommendation Systems: Instead of simple content-based recommendations, incorporate algorithms like collaborative filtering or hybrid approaches to make the system more sophisticated.\n",
    "    * Keyword search\n",
    "    * Graph-based\n",
    "* Search Functionality: Implement a fallback mechanism to search the internet if the requested information is not available in your data.\n",
    "* Experiment Tracking: Use MLFlow to log and track your experiments for better reproducibility and monitoring.\n",
    "* CI/CD Pipelines: As you already plan to implement CI/CD pipelines, ensure it is robust and scalable.\n",
    "\n",
    "Additional Features to Explore\n",
    "* Computer Vision: Allow users to upload book cover images, and build a system to recommend similar books based on the cover.\n",
    "* Speech Capabilities: Integrate speech-to-text and text-to-speech functionalities so users can interact with your system through voice commands, and the system can respond audibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b703d2-9834-486e-9bed-823005544774",
   "metadata": {},
   "source": [
    "# <u>Next steps</u>\n",
    "* Streamlit Cloud deployment fails. Packages install but then it complains about ModuleNotFoundError. Go through the logs and see if you can figure it out.\n",
    "* <s>Successfully deployed containerized Streamlit app to EC2; accessed via web browser</s>\n",
    "* <s>Create FastAPI backend for your app</s>\n",
    "    * With FastAPI, profile your code to determine performance (speed)\n",
    "    * <s>`lifespan` with single instance of `BookRetriever` and `BookAssistant` and `Depends` to inject</s>\n",
    "    * Websockets\n",
    "    * <s>`async`</s>\n",
    "* <s>Create Streamlit frontend to pair with FastAPI app</s>\n",
    "* <s>Containerize frontend and backend using Docker Compose. Use pip for package management. Deploy to EC2.</s>\n",
    "* Unittest/pytest + GitHub Actions\n",
    "* metadata filtering\n",
    "* Can I use regex with metadata filtering?\n",
    "* Should my backend return a status code of 200 as well?\n",
    "\n",
    "# <u>Questions</u>\n",
    "* HEALTHCHECK - even though my container is running fine, status is unhealthy\n",
    "* Push multiple images to one ECR repository?\n",
    "* Should I create a BookAssistant for each user? Or would it be sufficient to create one BookAssistant with async methods? Calling OpenAI and Qdrant APIs asynchronously? Websockets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358a8d7",
   "metadata": {},
   "source": [
    "```python\n",
    "backend/\n",
    "├── __init__.py\n",
    "├── dependencies.py     # Define dependency functions for BookAssistant, BookRetriever\n",
    "├── main.py             # Instantiate FastAPI with lifespan function. Lifespan function will initialize BookAssistant, BookRetriever.\n",
    "├── models\n",
    "│   ├── __init__.py\n",
    "│   ├── assistant.py    # Define BookAssistant class. For methods that call OpenAI API, use async/await. One instance will be shared across all users; simply update messages_list per user. Store convo histories in dict.\n",
    "│   └── retriever.py    # Define BookRetriever class. For methods that call Qdrant/OpenAI APIs, use async/await. One instance will be shared across all users.\n",
    "├── routers\n",
    "│   ├── __init__.py\n",
    "│   ├── chat.py         # One path operation function for both retrieval and chatting. Inject BookAssistant and BookRetriever objects and use Websocket for client-server communication.\n",
    "│   └── retrieve.py     # Deprecated\n",
    "└── services            # Don't need this\n",
    "    ├── __init__.py\n",
    "    └── vectordb.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9fcd5a",
   "metadata": {},
   "source": [
    "```graphql\n",
    "project/\n",
    "│\n",
    "├── backend/\n",
    "│   ├── __init__.py\n",
    "│   ├── main.py          # Entry point for FastAPI application\n",
    "│   ├── models/          # Chatbot and data-related classes\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── chatbot.py   # Chatbot class definition\n",
    "│   │   └── embedding.py # Query embedding utilities\n",
    "│   ├── services/        # Logic to interact with the vector DB and OpenAI API\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── vector_db.py # Interfacing with vector DB\n",
    "│   │   ├── openai_api.py # Interfacing with OpenAI APIs\n",
    "│   └── routers/         # API routes\n",
    "│       ├── __init__.py\n",
    "│       ├── retrieve.py  # Retrieval logic\n",
    "│       └── chat.py      # WebSocket logic\n",
    "│\n",
    "├── frontend/            # Optional: Streamlit or React/JS frontend\n",
    "│   ├── app.py           # Streamlit application\n",
    "│\n",
    "├── tests/               # Unit and integration tests\n",
    "│   ├── test_retrieve.py\n",
    "│   ├── test_chat.py\n",
    "│   └── ...\n",
    "└── requirements.txt     # Python dependencies\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f261c7f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
