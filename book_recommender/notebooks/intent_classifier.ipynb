{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/blogs/machine-learning/create-and-fine-tune-sentence-transformers-for-enhanced-classification-accuracy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch import nn, optim\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score, Specificity, AUROC, ROC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a pretrained Sentence Transformer model\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "# embedding_size = 384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max seq len = 256\n",
      "<class 'numpy.ndarray'>\n",
      "(2, 384)\n",
      "tensor([[1.0000, 0.9287],\n",
      "        [0.9287, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Max seq len = {model.max_seq_length}\")\n",
    "# sentences = [\"This is sentence 1.\",\n",
    "#              \"This is sentence 2.\"]\n",
    "\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(type(embeddings))\n",
    "# print(embeddings.shape)\n",
    "\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompts\n",
    "max seq len and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9\n",
    "# embedding_size = model.get_sentence_embedding_dimension()\n",
    "embedding_size = 384\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_size: int, num_classes: int):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_size, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor):\n",
    "        x = features[\"sentence_embedding\"]\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class SentenceTransformerWithHead(nn.Module):\n",
    "    def __init__(self, transformer: SentenceTransformer, head: ClassificationHead):\n",
    "        super(SentenceTransformerWithHead, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.head = head\n",
    "    \n",
    "    def forward(self, input: dict[torch.Tensor, torch.Tensor]):\n",
    "        # input['input_ids']\n",
    "        # input['attention_mask']\n",
    "        features = self.transformer(input)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "\n",
    "# head = ClassificationHead(embedding_size, num_classes)\n",
    "# model_with_head = SentenceTransformerWithHead(model, head)\n",
    "# head.to(device)\n",
    "# model_with_head.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# print(next(model.parameters()).device)\n",
    "# print(next(head.parameters()).device)\n",
    "# print(next(model_with_head.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 6251, 1015, 1012,  102],\n",
       "         [ 101, 2023, 2003, 6251, 1016, 1012,  102]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = model.tokenize(sentences)\n",
    "# inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = model_with_head(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, csv_file: str | None=None, df: pd.core.frame.DataFrame | None=None):\n",
    "        if csv_file is not None:\n",
    "            self.df = pd.read_csv(csv_file)\n",
    "        elif df is not None:\n",
    "            self.df = df\n",
    "        self.df = self.df.astype({\"Label\": \"int64\"})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        item = {\n",
    "            \"Query\": row[\"Query\"],\n",
    "            \"Label\": row[\"Label\"],\n",
    "\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"intent_classifier_samples.csv\")\n",
    "\n",
    "train_idx, val_idx = train_test_split(range(len(df)), test_size=0.3, stratify=df[\"Label\"], random_state=0)\n",
    "df_train = df.iloc[train_idx]\n",
    "val_idx, test_idx = train_test_split(val_idx, test_size=0.5, stratify=df.iloc[val_idx][\"Label\"], random_state=0)\n",
    "df_val = df.iloc[val_idx]\n",
    "df_test = df.iloc[test_idx]\n",
    "# print(df_train[\"Label\"].value_counts())\n",
    "# print(df_val[\"Label\"].value_counts())\n",
    "# print(df_test[\"Label\"].value_counts())\n",
    "# print(df_train.head())\n",
    "# print(df_val.head())\n",
    "# print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = QueryDataset(df=df_train)\n",
    "ds_val = QueryDataset(df=df_val)\n",
    "ds_test = QueryDataset(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Query': ['In the mood for a gripping thriller series with unexpected plot twists and morally ambiguous characters, any recommendations?',\n",
       "  \"I'm content with my literary picks for now, so I'll skip on any new recommendations.\",\n",
       "  \"Hey there, I'm in the mood for a book that will challenge my beliefs. Can you recommend something that will provoke my thoughts?\",\n",
       "  'Can you recommend a book that evokes emotions like Where the Crawdads Sing by Delia Owens?',\n",
       "  \"Just finished The Night Circus and I'm spellbound! Any recommendations for a book with a similarly enchanting and magical atmosphere?\",\n",
       "  'Can you recommend historical fiction books spotlighting the lives of extraordinary women throughout different eras in history?',\n",
       "  'How does your system handle user inquiries regarding the rationale behind specific book recommendations?',\n",
       "  'Do you have books in other languages?',\n",
       "  \"Let's take a moment to revisit the books we've touched upon earlier, shall we?\",\n",
       "  'What percentage of books in the Science Fiction genre have been adapted into movies based on your dataset?',\n",
       "  'Can you recommend a book that provokes deep reflection like The Alchemist? Seeking a soul-stirring journey of self-discovery.',\n",
       "  \"I'm actually taking a break from book recommendations, so I'll pass on any new suggestions for now.\",\n",
       "  \"Good evening! I'm searching for a book that will make me reflect on life's complexities. Can you recommend something deep and meaningful?\",\n",
       "  \"Could you enlighten me on the books we've previously mentioned?\",\n",
       "  \"Let's recap the books we've exchanged thoughts on in previous conversations.\",\n",
       "  \"Appreciate the thought, but I'm not quite ready for new book suggestions at this time.\",\n",
       "  \"I'm just exploring on my own, so no need for recommendations.\",\n",
       "  \"Let's revisit our past book chats, what titles stood out to you?\",\n",
       "  'Can you recommend historical fiction books spotlighting the lives of extraordinary women from diverse backgrounds throughout history?',\n",
       "  'Could you point me towards some highly-rated mystery novels for a cozy weekend read?',\n",
       "  \"Can you give me a brief overview of the noteworthy book titles we've touched on?\",\n",
       "  \"Let's reminisce on the books we've covered in our previous talks.\",\n",
       "  \"I'm looking for more novels similar to the ones I've recently enjoyed. Any recommendations?\",\n",
       "  \"I'm on the lookout for a captivating series with strong female characters. Any recommendations?\",\n",
       "  'Not eager for any book suggestions at the moment, just enjoying my literary journey.',\n",
       "  'What are some must-read fantasy novels introducing unique magical systems and intricate world-building elements?',\n",
       "  'Looking for contemporary novels that explore the complexities of human nature, any picks come to mind?',\n",
       "  'How do you ensure the transparency of the recommendation process to users?',\n",
       "  'Can you suggest any unique ways to organize a home workspace for better productivity?',\n",
       "  'How do reader reviews differ between standalone and series novels in the Humor category, and what factors contribute to these distinctions?',\n",
       "  \"I'm curious, what book titles were part of our past conversations about books?\",\n",
       "  \"Hey, I'm curious about exploring more thought-provoking philosophy books. Any deep thinkers you recommend diving into next?\",\n",
       "  'Do you believe in the benefits of practicing mindfulness for overall well-being?',\n",
       "  'Do you have novels emphasizing the importance of environmental sustainability and conservation efforts?',\n",
       "  'Can you recommend any contemporary novels that are similar to the ones we discussed previously?',\n",
       "  \"What's the book situation from our past discussions?\",\n",
       "  'Do you carry novels emphasizing the importance of environmental sustainability and conservation efforts?',\n",
       "  'How does the average publication year vary between Novels and Historical Fiction books in your dataset?',\n",
       "  \"I'm on a temporary pause from adding to my reading list, no need for suggestions right now.\",\n",
       "  \"Good day! I'm in the mood for a book that will make me ponder the mysteries of existence. Any suggestions for a philosophical and introspective read?\",\n",
       "  'Can users actively contribute to shaping the genres and types of books recommended to them?',\n",
       "  'I need a book that immerses me in a magical world akin to The Night Circus. Any recommendations for a captivating read that blends fantasy and enchantment?',\n",
       "  'What are some top-rated science fiction books that delve into the consequences of genetic manipulation and bioengineering?',\n",
       "  'Show me a comparison of book sales figures for self-published authors versus traditionally published writers in the Contemporary category.',\n",
       "  \"After experiencing the enchanting allure of The Night Circus, I'm eager to dive into another book that immerses me in a world of enchantment. Any suggestions for a captivating read with a magical twist?\",\n",
       "  \"Do you think it's better to read a book in physical form or on an e-reader?\",\n",
       "  \"After reading The Alchemist, I'm seeking another spiritually uplifting book. Any recommendations for a soul-stirring read?\",\n",
       "  'Currently satisfied with my reading material, no need for new recommendations.',\n",
       "  'Not looking to add to my reading list right now, but I appreciate the offer.',\n",
       "  'Seeking novels that transport readers to remote villages and immerse them in traditional customs. Any literary escapes you recommend?',\n",
       "  \"Good day! I'm searching for a book that will make me laugh and cry in equal measure. Any recommendations?\",\n",
       "  'I devoured the humor in the last book you suggested. Any more light-hearted reads to brighten my day?',\n",
       "  \"I'm looking for highly-rated mystery novels for a cozy weekend read, any suggestions?\",\n",
       "  \"Good day! I'm searching for a book that will keep me on the edge of my seat. Any suggestions for a gripping and suspenseful read?\",\n",
       "  'What trends can you spot in reader feedback for books published in the last 5 years across all categories?',\n",
       "  'Hi there, how do you feel about the idea of adopting a furry friend from a shelter?',\n",
       "  'Hey, what are the most popular nonfiction books discussing the impact of climate change on global ecosystems?',\n",
       "  \"I'm trying to remember, what were the book recommendations from our previous chats?\",\n",
       "  \"Hey, I'm thirsty for more adventure after finishing the last book. Any exhilarating journeys awaiting me in the pages of a new story?\",\n",
       "  \"Good day! I'm hunting for a book that will make me ponder the mysteries of existence. Any recommendations for a philosophical read?\",\n",
       "  'How does your book recommender application adapt to my changing reading preferences over time?',\n",
       "  'Do you believe in the power of aromatherapy for stress relief and relaxation?',\n",
       "  \"I'm content with my literary choices at the moment, so no need for new recommendations.\",\n",
       "  \"Thanks, but I'm currently content with my reading list, no need for additional suggestions right now.\"],\n",
       " 'Label': tensor([0, 6, 2, 1, 1, 0, 7, 8, 4, 8, 1, 6, 2, 4, 4, 6, 6, 4, 0, 0, 4, 4, 3, 0,\n",
       "         6, 0, 0, 7, 5, 8, 4, 3, 5, 0, 3, 4, 0, 8, 6, 2, 7, 1, 0, 8, 1, 5, 1, 6,\n",
       "         6, 0, 2, 3, 0, 2, 8, 5, 0, 4, 3, 2, 7, 5, 6, 6])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "sample = next(iter(train_loader))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"Query\"]\n",
    "# sample[\"Label\"]\n",
    "# model.tokenize(sample[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformerWithHead(\n",
       "  (transformer): SentenceTransformer(\n",
       "    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "    (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "    (2): Normalize()\n",
       "  )\n",
       "  (head): ClassificationHead(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "head = ClassificationHead(embedding_size, num_classes)\n",
    "model_with_head = SentenceTransformerWithHead(model, head)\n",
    "head.to(device)\n",
    "model_with_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_with_head.parameters()).device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SentenceTransformer.tokenize of SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_head.transformer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.layers.0.weight\n",
      "head.layers.0.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze all weights except classifier head\n",
    "for name, param in model_with_head.named_parameters():\n",
    "    # print(name)\n",
    "    if not re.search(r\"^head\", name):\n",
    "        # print(name)\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, param in model_with_head.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MulticlassAccuracy'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "f1.to(device)\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "accuracy.to(device)\n",
    "f1._get_name()\n",
    "accuracy._get_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.75, Train Accuracy: 0.40, Train F1: 0.43,           Val Loss: 1.36, Val Accuracy: 0.68, Val F1: 0.49\n",
      "Train Loss: 1.10, Train Accuracy: 0.83, Train F1: 0.67,           Val Loss: 0.91, Val Accuracy: 0.84, Val F1: 0.69\n",
      "Train Loss: 0.76, Train Accuracy: 0.86, Train F1: 0.75,           Val Loss: 0.69, Val Accuracy: 0.91, Val F1: 0.76\n",
      "Train Loss: 0.57, Train Accuracy: 0.93, Train F1: 0.80,           Val Loss: 0.56, Val Accuracy: 0.92, Val F1: 0.81\n",
      "Train Loss: 0.46, Train Accuracy: 0.94, Train F1: 0.83,           Val Loss: 0.47, Val Accuracy: 0.92, Val F1: 0.84\n",
      "Train Loss: 0.39, Train Accuracy: 0.94, Train F1: 0.85,           Val Loss: 0.41, Val Accuracy: 0.93, Val F1: 0.86\n",
      "Train Loss: 0.34, Train Accuracy: 0.95, Train F1: 0.87,           Val Loss: 0.37, Val Accuracy: 0.95, Val F1: 0.87\n",
      "Train Loss: 0.30, Train Accuracy: 0.96, Train F1: 0.88,           Val Loss: 0.34, Val Accuracy: 0.95, Val F1: 0.88\n",
      "Train Loss: 0.27, Train Accuracy: 0.96, Train F1: 0.89,           Val Loss: 0.31, Val Accuracy: 0.95, Val F1: 0.89\n",
      "Train Loss: 0.24, Train Accuracy: 0.96, Train F1: 0.90,           Val Loss: 0.29, Val Accuracy: 0.96, Val F1: 0.90\n",
      "Train Loss: 0.22, Train Accuracy: 0.97, Train F1: 0.91,           Val Loss: 0.27, Val Accuracy: 0.96, Val F1: 0.91\n",
      "Train Loss: 0.20, Train Accuracy: 0.97, Train F1: 0.91,           Val Loss: 0.25, Val Accuracy: 0.95, Val F1: 0.91\n",
      "Train Loss: 0.19, Train Accuracy: 0.98, Train F1: 0.92,           Val Loss: 0.24, Val Accuracy: 0.96, Val F1: 0.92\n",
      "Train Loss: 0.17, Train Accuracy: 0.98, Train F1: 0.92,           Val Loss: 0.23, Val Accuracy: 0.96, Val F1: 0.92\n",
      "Train Loss: 0.16, Train Accuracy: 0.98, Train F1: 0.93,           Val Loss: 0.22, Val Accuracy: 0.97, Val F1: 0.93\n",
      "Train Loss: 0.15, Train Accuracy: 0.98, Train F1: 0.93,           Val Loss: 0.21, Val Accuracy: 0.96, Val F1: 0.93\n",
      "Train Loss: 0.14, Train Accuracy: 0.99, Train F1: 0.93,           Val Loss: 0.20, Val Accuracy: 0.97, Val F1: 0.93\n",
      "Train Loss: 0.13, Train Accuracy: 0.99, Train F1: 0.94,           Val Loss: 0.19, Val Accuracy: 0.97, Val F1: 0.94\n",
      "Train Loss: 0.13, Train Accuracy: 0.99, Train F1: 0.94,           Val Loss: 0.19, Val Accuracy: 0.97, Val F1: 0.94\n",
      "Train Loss: 0.12, Train Accuracy: 0.99, Train F1: 0.94,           Val Loss: 0.18, Val Accuracy: 0.97, Val F1: 0.94\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.AdamW(model_with_head.parameters(), lr=0.01) # 0.01 for head only, 0.0001 for all weights\n",
    "# lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.999)\n",
    "\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "f1.to(device)\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "accuracy.to(device)\n",
    "\n",
    "metrics = [accuracy, f1]\n",
    "\n",
    "n_epochs = 20\n",
    "training_results = {\n",
    "    \"Train Loss\": np.ones(n_epochs)*-999,\n",
    "    \"Train Accuracy\": np.ones(n_epochs)*-999,\n",
    "    \"Train F1\": np.ones(n_epochs)*-999,\n",
    "    \"Val Loss\": np.ones(n_epochs)*-999,\n",
    "    \"Val Accuracy\": np.ones(n_epochs)*-999,\n",
    "    \"Val F1\": np.ones(n_epochs)*-999,\n",
    "}\n",
    "for edx in range(n_epochs):\n",
    "    cum_loss = 0\n",
    "    accuracy.reset()\n",
    "    for bdx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = model.tokenize(batch[\"Query\"])\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        labels = batch[\"Label\"].to(device)\n",
    "\n",
    "        logits = model_with_head(inputs)\n",
    "\n",
    "        loss = loss_function(logits, labels)\n",
    "        cum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric.update(torch.argmax(logits, dim=1), labels)\n",
    "        # accuracy.update(torch.argmax(logits, dim=1), labels)\n",
    "    \n",
    "    t_loss = cum_loss/len(train_loader.dataset.df)\n",
    "    t_acc = accuracy.compute().detach().cpu().numpy().item()\n",
    "    t_f1 = f1.compute().detach().cpu().numpy().item()\n",
    "    training_results[\"Train Loss\"][edx] = t_loss\n",
    "    training_results[\"Train Accuracy\"][edx] = t_acc\n",
    "    training_results[\"Train F1\"][edx] = t_f1\n",
    "\n",
    "    cum_loss = 0\n",
    "    accuracy.reset()\n",
    "    with torch.no_grad():\n",
    "        for bdx, batch in enumerate(val_loader):\n",
    "            inputs = model.tokenize(batch[\"Query\"])\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "            labels = batch[\"Label\"].to(device)\n",
    "\n",
    "            logits = model_with_head(inputs)\n",
    "\n",
    "            loss = loss_function(logits, labels)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "            for metric in metrics:\n",
    "                metric.update(torch.argmax(logits, dim=1), labels)\n",
    "    \n",
    "    v_loss = cum_loss/len(val_loader.dataset.df)\n",
    "    v_acc = accuracy.compute().detach().cpu().numpy().item()\n",
    "    v_f1 = f1.compute().detach().cpu().numpy().item()\n",
    "    training_results[\"Val Loss\"][edx] = v_loss\n",
    "    training_results[\"Val Accuracy\"][edx] = v_acc\n",
    "    training_results[\"Val F1\"][edx] = v_f1\n",
    "\n",
    "    print(f\"Train Loss: {t_loss:.2f}, Train Accuracy: {t_acc:.2f}, Train F1: {t_f1:.2f}, \\\n",
    "          Val Loss: {v_loss:.2f}, Val Accuracy: {v_acc:.2f}, Val F1: {v_f1:.2f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 7, 7, 7, 0, 7, 7, 7, 5, 3, 6, 7, 6, 7, 7], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
