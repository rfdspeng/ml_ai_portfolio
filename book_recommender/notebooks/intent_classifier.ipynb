{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/blogs/machine-learning/create-and-fine-tune-sentence-transformers-for-enhanced-classification-accuracy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "embedding_size = 384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max seq len = 256\n",
      "<class 'numpy.ndarray'>\n",
      "(2, 384)\n",
      "tensor([[1.0000, 0.9287],\n",
      "        [0.9287, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max seq len = {model.max_seq_length}\")\n",
    "sentences = [\"This is sentence 1.\",\n",
    "             \"This is sentence 2.\"]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(type(embeddings))\n",
    "print(embeddings.shape)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformerWithHead(\n",
       "  (transformer): SentenceTransformer(\n",
       "    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "    (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "    (2): Normalize()\n",
       "  )\n",
       "  (head): ClassificationHead(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "num_classes = 9\n",
    "embedding_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_size: int, num_classes: int):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_size, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor):\n",
    "        x = features[\"sentence_embedding\"]\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class SentenceTransformerWithHead(nn.Module):\n",
    "    def __init__(self, transformer: SentenceTransformer, head: ClassificationHead):\n",
    "        super(SentenceTransformerWithHead, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.head = head\n",
    "    \n",
    "    def forward(self, input: dict[torch.Tensor, torch.Tensor]):\n",
    "        # input['input_ids']\n",
    "        # input['attention_mask']\n",
    "        features = self.transformer(input)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "\n",
    "head = ClassificationHead(embedding_size, num_classes)\n",
    "model_with_head = SentenceTransformerWithHead(model, head)\n",
    "head.to(device)\n",
    "model_with_head.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)\n",
    "print(next(head.parameters()).device)\n",
    "print(next(model_with_head.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 6251, 1015, 1012,  102],\n",
       "         [ 101, 2023, 2003, 6251, 1016, 1012,  102]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = model.tokenize(sentences)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_with_head(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.0.auto_model.embeddings.word_embeddings.weight\n",
      "transformer.0.auto_model.embeddings.position_embeddings.weight\n",
      "transformer.0.auto_model.embeddings.token_type_embeddings.weight\n",
      "transformer.0.auto_model.embeddings.LayerNorm.weight\n",
      "transformer.0.auto_model.embeddings.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.0.attention.self.query.weight\n",
      "transformer.0.auto_model.encoder.layer.0.attention.self.query.bias\n",
      "transformer.0.auto_model.encoder.layer.0.attention.self.key.weight\n",
      "transformer.0.auto_model.encoder.layer.0.attention.self.key.bias\n",
      "transformer.0.auto_model.encoder.layer.0.attention.self.value.weight\n",
      "transformer.0.auto_model.encoder.layer.0.attention.self.value.bias\n",
      "transformer.0.auto_model.encoder.layer.0.attention.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.0.attention.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.0.intermediate.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.0.intermediate.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.0.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.0.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.0.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.0.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.1.attention.self.query.weight\n",
      "transformer.0.auto_model.encoder.layer.1.attention.self.query.bias\n",
      "transformer.0.auto_model.encoder.layer.1.attention.self.key.weight\n",
      "transformer.0.auto_model.encoder.layer.1.attention.self.key.bias\n",
      "transformer.0.auto_model.encoder.layer.1.attention.self.value.weight\n",
      "transformer.0.auto_model.encoder.layer.1.attention.self.value.bias\n",
      "transformer.0.auto_model.encoder.layer.1.attention.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.1.attention.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.1.intermediate.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.1.intermediate.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.1.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.1.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.1.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.1.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.2.attention.self.query.weight\n",
      "transformer.0.auto_model.encoder.layer.2.attention.self.query.bias\n",
      "transformer.0.auto_model.encoder.layer.2.attention.self.key.weight\n",
      "transformer.0.auto_model.encoder.layer.2.attention.self.key.bias\n",
      "transformer.0.auto_model.encoder.layer.2.attention.self.value.weight\n",
      "transformer.0.auto_model.encoder.layer.2.attention.self.value.bias\n",
      "transformer.0.auto_model.encoder.layer.2.attention.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.2.attention.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.2.intermediate.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.2.intermediate.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.2.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.2.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.2.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.2.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.3.attention.self.query.weight\n",
      "transformer.0.auto_model.encoder.layer.3.attention.self.query.bias\n",
      "transformer.0.auto_model.encoder.layer.3.attention.self.key.weight\n",
      "transformer.0.auto_model.encoder.layer.3.attention.self.key.bias\n",
      "transformer.0.auto_model.encoder.layer.3.attention.self.value.weight\n",
      "transformer.0.auto_model.encoder.layer.3.attention.self.value.bias\n",
      "transformer.0.auto_model.encoder.layer.3.attention.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.3.attention.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.3.intermediate.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.3.intermediate.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.3.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.3.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.3.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.3.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.4.attention.self.query.weight\n",
      "transformer.0.auto_model.encoder.layer.4.attention.self.query.bias\n",
      "transformer.0.auto_model.encoder.layer.4.attention.self.key.weight\n",
      "transformer.0.auto_model.encoder.layer.4.attention.self.key.bias\n",
      "transformer.0.auto_model.encoder.layer.4.attention.self.value.weight\n",
      "transformer.0.auto_model.encoder.layer.4.attention.self.value.bias\n",
      "transformer.0.auto_model.encoder.layer.4.attention.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.4.attention.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.4.intermediate.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.4.intermediate.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.4.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.4.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.4.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.4.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.5.attention.self.query.weight\n",
      "transformer.0.auto_model.encoder.layer.5.attention.self.query.bias\n",
      "transformer.0.auto_model.encoder.layer.5.attention.self.key.weight\n",
      "transformer.0.auto_model.encoder.layer.5.attention.self.key.bias\n",
      "transformer.0.auto_model.encoder.layer.5.attention.self.value.weight\n",
      "transformer.0.auto_model.encoder.layer.5.attention.self.value.bias\n",
      "transformer.0.auto_model.encoder.layer.5.attention.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.5.attention.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "transformer.0.auto_model.encoder.layer.5.intermediate.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.5.intermediate.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.5.output.dense.weight\n",
      "transformer.0.auto_model.encoder.layer.5.output.dense.bias\n",
      "transformer.0.auto_model.encoder.layer.5.output.LayerNorm.weight\n",
      "transformer.0.auto_model.encoder.layer.5.output.LayerNorm.bias\n",
      "transformer.0.auto_model.pooler.dense.weight\n",
      "transformer.0.auto_model.pooler.dense.bias\n",
      "head.layers.0.weight\n",
      "head.layers.0.bias\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Freeze all weights except classifier head\n",
    "# for name, param in model_with_head.named_parameters():\n",
    "#     # print(name)\n",
    "#     if not re.search(r\"^head\", name):\n",
    "#         # print(name)\n",
    "#         param.requires_grad = False\n",
    "\n",
    "for name, param in model_with_head.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        item = {\n",
    "            \"Query\": row[\"Query\"],\n",
    "            \"Label\": row[\"Label\"],\n",
    "\n",
    "        }\n",
    "        return item\n",
    "\n",
    "ds = QueryDataset(\"intent_classifier_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2025,  2559,  2005,  2151,  3752, 11433,  2747,  1010,  2021,\n",
       "           4283,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2071,  2017,  3073, 20062,  2046,  1996,  8906,  1997,  6048,\n",
       "           1999,  2115,  2951, 13462,  1029,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2054,  2051,  2515,  1996,  3075,  2485,  2651,  1029,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1045,  1005,  1049,  8025,  2055,  1996,  4353,  1997,  4772,\n",
       "           2086,  1999,  2115,  2951, 13462,  1012,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  4931,  2045,  1010,  1045,  1005,  1049,  2559,  2005,  1037,\n",
       "           2338,  2008,  2097,  2562,  2033, 13322,  1012,  2064,  2017,  6592,\n",
       "           2242,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1045,  1005,  1049,  1999,  1996,  6888,  2005,  1037, 13940,\n",
       "           6547,  3117,  1012,  2151, 11433,  2206,  1996,  3924,  2017,  2435,\n",
       "           2033,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2129,  2116,  2808,  2031,  2180,  4706,  2982,  1999,  2115,\n",
       "           3074,  1029,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1045,  1005,  1049,  5059,  1037,  8744,  1010,  2064,  2017,\n",
       "           2862,  1996,  2808,  2057,  1005,  2310,  5028,  2006,  1029,   102,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1045,  1005,  1049,  2986,  2302,  2151,  2338, 11433,  2005,\n",
       "           2085,  1010,  4283,  1012,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2204,  2851,   999,  1045,  2215,  2000,  3972,  3726,  2046,\n",
       "           1037,  2047,  2338,  1012,  2064,  2017, 16755,  2242, 11973,  1029,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  8016,  2033,  1010,  2071,  2017,  2391,  2033,  2875,  1037,\n",
       "           2307,  2338,  2000,  3191,  1029,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1045,  1005,  1049,  2635,  1037,  3338,  2013,  2338, 15690,\n",
       "           1010,  2061,  2053,  2342,  2000,  3745,  1012,   102,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  2054,  1005,  1055,  1996,  2190,  2126,  2000,  9483,  2044,\n",
       "           1037,  2146,  2154,  1029,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1045,  2064,  1005,  1056,  3342,  1010,  2054,  2808,  2031,\n",
       "           2057,  6936,  2525,  1029,   102,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  4931,  1010,  1045,  1005,  1049,  9461,  2000,  2707,  1037,\n",
       "           2047,  2338,  1012,  2064,  2017,  2393,  2033,  2424,  2028,  1029,\n",
       "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0],\n",
       "         [  101,  1045,  1005,  1049,  6575,  2005,  1037,  2338,  2008,  2038,\n",
       "           1996,  2168,  4254,  2004,  1996,  2338, 12383,  2011, 23030, 16950,\n",
       "           3736,  2243,  1012,  2151, 11433,  2005,  1037,  3048,  3439,  4349,\n",
       "           3191,  1029,   102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"Query\"]\n",
    "sample[\"Label\"]\n",
    "model.tokenize(sample[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.AdamW(model_with_head.parameters(), lr=0.001)\n",
    "# lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "\n",
    "n_epochs = 30\n",
    "for edx in range(n_epochs):\n",
    "    cum_loss = 0\n",
    "    for bdx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = model.tokenize(batch[\"Query\"])\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        labels = batch[\"Label\"].to(device)\n",
    "\n",
    "        logits = model_with_head(inputs)\n",
    "\n",
    "        loss = loss_function(logits, labels)\n",
    "        cum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "    print(cum_loss/len(train_dataloader.dataset.df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
