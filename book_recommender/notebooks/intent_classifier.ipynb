{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/blogs/machine-learning/create-and-fine-tune-sentence-transformers-for-enhanced-classification-accuracy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchmetrics\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch import nn, optim\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score, Specificity, AUROC, ROC\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a pretrained Sentence Transformer model\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "# embedding_size = 384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max seq len = 256\n",
      "<class 'numpy.ndarray'>\n",
      "(2, 384)\n",
      "tensor([[1.0000, 0.9287],\n",
      "        [0.9287, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Max seq len = {model.max_seq_length}\")\n",
    "# sentences = [\"This is sentence 1.\",\n",
    "#              \"This is sentence 2.\"]\n",
    "\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(type(embeddings))\n",
    "# print(embeddings.shape)\n",
    "\n",
    "# similarities = model.similarity(embeddings, embeddings)\n",
    "# print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompts\n",
    "max seq len and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9\n",
    "# embedding_size = model.get_sentence_embedding_dimension()\n",
    "embedding_size = 384\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_size: int, num_classes: int):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_size, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, features: torch.Tensor):\n",
    "        x = features[\"sentence_embedding\"]\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class SentenceTransformerWithHead(nn.Module):\n",
    "    def __init__(self, transformer: SentenceTransformer, head: ClassificationHead):\n",
    "        super(SentenceTransformerWithHead, self).__init__()\n",
    "        self.transformer = transformer\n",
    "        self.head = head\n",
    "    \n",
    "    def forward(self, input: dict[torch.Tensor, torch.Tensor]):\n",
    "        # input['input_ids']\n",
    "        # input['attention_mask']\n",
    "        features = self.transformer(input)\n",
    "        logits = self.head(features)\n",
    "        return logits\n",
    "\n",
    "# head = ClassificationHead(embedding_size, num_classes)\n",
    "# model_with_head = SentenceTransformerWithHead(model, head)\n",
    "# head.to(device)\n",
    "# model_with_head.to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# print(next(model.parameters()).device)\n",
    "# print(next(head.parameters()).device)\n",
    "# print(next(model_with_head.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 6251, 1015, 1012,  102],\n",
       "         [ 101, 2023, 2003, 6251, 1016, 1012,  102]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = model.tokenize(sentences)\n",
    "# inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = model_with_head(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.df = self.df.astype({\"Label\": \"int64\"})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        item = {\n",
    "            \"Query\": row[\"Query\"],\n",
    "            \"Label\": row[\"Label\"],\n",
    "\n",
    "        }\n",
    "        return item\n",
    "\n",
    "ds = QueryDataset(\"intent_classifier_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query       object\n",
       "Label        int64\n",
       "Notes       object\n",
       "Category    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[\"Query\"]\n",
    "# sample[\"Label\"]\n",
    "# model.tokenize(sample[\"Query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformerWithHead(\n",
       "  (transformer): SentenceTransformer(\n",
       "    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "    (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "    (2): Normalize()\n",
       "  )\n",
       "  (head): ClassificationHead(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=9, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "head = ClassificationHead(embedding_size, num_classes)\n",
    "model_with_head = SentenceTransformerWithHead(model, head)\n",
    "head.to(device)\n",
    "model_with_head.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.layers.0.weight\n",
      "head.layers.0.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze all weights except classifier head\n",
    "for name, param in model_with_head.named_parameters():\n",
    "    # print(name)\n",
    "    if not re.search(r\"^head\", name):\n",
    "        # print(name)\n",
    "        param.requires_grad = False\n",
    "\n",
    "for name, param in model_with_head.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.18, Accuracy: 0.12\n",
      "Loss: 0.66, Accuracy: 0.40\n",
      "Loss: 0.48, Accuracy: 0.77\n",
      "Loss: 0.37, Accuracy: 0.91\n",
      "Loss: 0.30, Accuracy: 0.92\n",
      "Loss: 0.24, Accuracy: 0.96\n",
      "Loss: 0.21, Accuracy: 0.97\n",
      "Loss: 0.18, Accuracy: 0.98\n",
      "Loss: 0.15, Accuracy: 0.98\n",
      "Loss: 0.14, Accuracy: 0.99\n",
      "Loss: 0.12, Accuracy: 0.99\n",
      "Loss: 0.11, Accuracy: 0.99\n",
      "Loss: 0.10, Accuracy: 0.99\n",
      "Loss: 0.09, Accuracy: 0.99\n",
      "Loss: 0.08, Accuracy: 0.99\n",
      "Loss: 0.08, Accuracy: 0.99\n",
      "Loss: 0.07, Accuracy: 0.99\n",
      "Loss: 0.06, Accuracy: 0.99\n",
      "Loss: 0.06, Accuracy: 1.00\n",
      "Loss: 0.06, Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "optimizer = optim.AdamW(model_with_head.parameters(), lr=0.01) # 0.01 for head only, 0.0001 for all weights\n",
    "# lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.98)\n",
    "\n",
    "f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n",
    "accuracy.to(device)\n",
    "\n",
    "n_epochs = 20\n",
    "for edx in range(n_epochs):\n",
    "    cum_loss = 0\n",
    "    accuracy.reset()\n",
    "    for bdx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = model.tokenize(batch[\"Query\"])\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        labels = batch[\"Label\"].to(device)\n",
    "\n",
    "        logits = model_with_head(inputs)\n",
    "\n",
    "        loss = loss_function(logits, labels)\n",
    "        cum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # lr_scheduler.step()\n",
    "\n",
    "        accuracy.update(torch.argmax(logits, dim=1), labels)\n",
    "    \n",
    "    cum_loss /= len(train_dataloader.dataset.df)\n",
    "    # accuracy.compute()\n",
    "    print(f\"Loss: {cum_loss:.2f}, Accuracy: {accuracy.compute().detach().cpu().numpy().item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 7, 7, 7, 0, 7, 7, 7, 5, 3, 6, 7, 6, 7, 7], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
