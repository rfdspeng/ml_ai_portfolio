{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e4c964-4863-4121-9baf-354bddcdb4a5",
   "metadata": {},
   "source": [
    "# How to use huggingface tokenizers and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eda2df-55be-4fdf-8f88-a582a38ab065",
   "metadata": {},
   "source": [
    "### Some basic info about tokenization from this reference: https://huggingface.co/learn/nlp-course/en/chapter2/4\n",
    "* Word tokenization results in a very large vocabulary since you need a unique ID for every unique word. This can result in rare words being mapped to <UNK>, or unknown.\n",
    "* Character tokenization results in a very small vocabulary (at least for English), but the tokens aren't very meaningful. Also, the number of tokens that need to be processed by the model becomes very large.\n",
    "* Subword tokenization is a good balance. It reduces the vocabulary size while still maintaining meaningful tokens. Rare words can still be attended to because they are composed of common subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1cda75-b1fd-45d4-969d-aebb6acafb91",
   "metadata": {},
   "source": [
    "### <u>Let's first get a feel for what the tokenizer does.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5a7757-5252-47f2-89eb-46e6d4f71b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10952c9f-70b0-4f3a-8d0a-ffac4cdd2e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': tensor([[  101,  2043,  2017,  1005,  2128,  1996,  2684,  1997,  1996, 18955,\n",
      "          6396,  8737,  2232,  1010, 21856,  1010,  1998,  1037, 16537,  2170,\n",
      "         14412,  8523,  1010,  2017,  5987,  2000,  2022,  2445,  1037,  6881,\n",
      "          2171,  1012,   102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 33])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('cuda available:', torch.cuda.is_available())\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "\n",
    "text = r\"When you're the daughter of the Oceanic nymph, Styx, and a Titan called Pallas, you expect to be given a weird name.\"\n",
    "\n",
    "# text = r\"This is a test of run vs. running.\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt').to(device)\n",
    "print(type(encoded_input))\n",
    "print(encoded_input) # contains input_ids and attention_mask\n",
    "print(encoded_input['input_ids'].shape)\n",
    "print(encoded_input['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c0c4e-2ce3-4a69-b7c5-09621e26bda2",
   "metadata": {},
   "source": [
    "### <u>What about using the tokenizer to decode?</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43591873-83fb-486c-9e7e-066bef83f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] when you're the daughter of the oceanic nymph, styx, and a titan called pallas, you expect to be given a weird name. [SEP]\n",
      "when you're the daughter of the oceanic nymph, styx, and a titan called pallas, you expect to be given a weird name.\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded_input['input_ids'].squeeze())\n",
    "decoded_remove_special = tokenizer.decode(encoded_input['input_ids'].squeeze(), skip_special_tokens=True)\n",
    "print(decoded)\n",
    "print(decoded_remove_special)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638deccd-6a12-4a93-9e48-4a2f31273878",
   "metadata": {},
   "source": [
    "### <u>Let's see what the tokenizer is doing internally.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99260bc2-c02b-45c6-af2d-ae5617e9a642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'you', \"'\", 're', 'the', 'daughter', 'of', 'the', 'oceanic', 'ny', '##mp', '##h', ',', 'styx', ',', 'and', 'a', 'titan', 'called', 'pal', '##las', ',', 'you', 'expect', 'to', 'be', 'given', 'a', 'weird', 'name', '.']\n",
      "[2043, 2017, 1005, 2128, 1996, 2684, 1997, 1996, 18955, 6396, 8737, 2232, 1010, 21856, 1010, 1998, 1037, 16537, 2170, 14412, 8523, 1010, 2017, 5987, 2000, 2022, 2445, 1037, 6881, 2171, 1012]\n",
      "when you're the daughter of the oceanic nymph, styx, and a titan called pallas, you expect to be given a weird name.\n"
     ]
    }
   ],
   "source": [
    "# Generally, you won't be using these methods yourself.\n",
    "\n",
    "# These two methods comprise encoding: tokenizing text and converting to IDs\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens) # uses a vocabulary to do the conversion\n",
    "\n",
    "# Converts IDs back to tokens and groups them together into a readable sequence\n",
    "# Useful for decoding the output of generative models\n",
    "decoded = tokenizer.decode(ids)\n",
    "\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965c191-ef78-4547-a4e4-a0b1a36f16e1",
   "metadata": {},
   "source": [
    "### <u>Let's see how we pass the encoded input to a model.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d02cbab-1546-444f-9c47-16341dc54b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 33, 768)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\", torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "output = model(**encoded_input) # unpack the encoded input (ids, attention mask) and pass it to the model\n",
    "final_output = output.last_hidden_state.detach().cpu().numpy()\n",
    "print(final_output.shape) # batch size (1 in this case), number of tokens, and hidden size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8138490-04ee-4e22-8572-20ce38dd2c61",
   "metadata": {},
   "source": [
    "### *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b1cebd4-0c28-44e6-9e30-4f9b8b36296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mteb/reddit-clustering\") # download a small dataset for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87d09858-f41f-448c-a539-2053bb347d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['sentences', 'labels'],\n",
       "        num_rows: 25\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DatasetDict, e.g. ['train'], ['test']. This one only has 'test'.\n",
    "# DatasetDict['test'] is a dataset. It's a tabular object build on arrow table (slightly different from pandas DataFrame).\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309da65-0a60-4bea-afdf-20b3071b4f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab226f64-75f0-4330-b8da-ca8b76a989cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can convert Dataset to DataFrame\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "dataframe = pd.DataFrame(dataset['test'])\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806da78-95f8-40ba-a266-63b438730063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b4515-ce1b-4432-9cfd-e80f47e9e5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0d2b3-a735-4a31-94f2-9c7bea8f624d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
