Experiment tracking, model packaging, model serving

Extremely important tool for MLE - hyperparameter tuning, tracking model performance

Registering your models, packaging models for deployment

Other ways to serve models: SageMaker, Ray, Lambda, Flask, FastAPI, native PyTorch and TF. What exactly does model serving mean? It means creating a server.

MLFlow can be used for complete ML lifecycle. What does this mean?

